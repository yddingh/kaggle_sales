{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611f78f7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-26T09:33:38.279295Z",
     "iopub.status.busy": "2025-08-26T09:33:38.278963Z",
     "iopub.status.idle": "2025-08-26T09:33:40.079394Z",
     "shell.execute_reply": "2025-08-26T09:33:40.078301Z"
    },
    "papermill": {
     "duration": 1.805906,
     "end_time": "2025-08-26T09:33:40.080856",
     "exception": false,
     "start_time": "2025-08-26T09:33:38.274950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/store-sales-time-series-forecasting/oil.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/stores.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/train.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/test.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/transactions.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689cff15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T09:33:40.087304Z",
     "iopub.status.busy": "2025-08-26T09:33:40.086913Z",
     "iopub.status.idle": "2025-08-26T09:33:43.624361Z",
     "shell.execute_reply": "2025-08-26T09:33:43.623529Z"
    },
    "papermill": {
     "duration": 3.542241,
     "end_time": "2025-08-26T09:33:43.626066",
     "exception": false,
     "start_time": "2025-08-26T09:33:40.083825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input dataset\n",
    "train = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\n",
    "stores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\n",
    "holidays = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv', parse_dates=['date'])\n",
    "oil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv', parse_dates=['date'])\n",
    "transactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84786b34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T09:33:43.633334Z",
     "iopub.status.busy": "2025-08-26T09:33:43.633004Z",
     "iopub.status.idle": "2025-08-26T09:33:46.790231Z",
     "shell.execute_reply": "2025-08-26T09:33:46.789316Z"
    },
    "papermill": {
     "duration": 3.162217,
     "end_time": "2025-08-26T09:33:46.791744",
     "exception": false,
     "start_time": "2025-08-26T09:33:43.629527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3054348 entries, 0 to 3054347\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   id               int64         \n",
      " 1   date             datetime64[ns]\n",
      " 2   store_nbr        int64         \n",
      " 3   family           object        \n",
      " 4   sales            float64       \n",
      " 5   onpromotion      int64         \n",
      " 6   city             object        \n",
      " 7   state            object        \n",
      " 8   type             object        \n",
      " 9   cluster          int64         \n",
      " 10  transactions     float64       \n",
      " 11  dcoilwtico       float64       \n",
      " 12  holidays_locale  object        \n",
      " 13  transferred      object        \n",
      "dtypes: datetime64[ns](1), float64(3), int64(4), object(6)\n",
      "memory usage: 326.2+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28512 entries, 0 to 28511\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   id               28512 non-null  int64         \n",
      " 1   date             28512 non-null  datetime64[ns]\n",
      " 2   store_nbr        28512 non-null  int64         \n",
      " 3   family           28512 non-null  object        \n",
      " 4   onpromotion      28512 non-null  int64         \n",
      " 5   city             28512 non-null  object        \n",
      " 6   state            28512 non-null  object        \n",
      " 7   type             28512 non-null  object        \n",
      " 8   cluster          28512 non-null  int64         \n",
      " 9   transactions     0 non-null      float64       \n",
      " 10  dcoilwtico       21384 non-null  float64       \n",
      " 11  holidays_locale  1782 non-null   object        \n",
      " 12  transferred      1782 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(4), object(6)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# adjust date formats\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "transactions['date'] = pd.to_datetime(transactions['date'])\n",
    "holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "oil['date'] = pd.to_datetime(oil['date'])\n",
    "\n",
    "# merge the dataset\n",
    "train = train.merge(stores, on='store_nbr', how='left')\n",
    "test = test.merge(stores, on='store_nbr', how='left')\n",
    "\n",
    "train = train.merge(transactions, on=['date', 'store_nbr'], how='left')\n",
    "test = test.merge(transactions, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "train = train.merge(oil, on='date', how='left')\n",
    "test = test.merge(oil, on='date', how='left')\n",
    "\n",
    "holidays = holidays[['date', 'locale', 'transferred']].rename(columns={'locale': 'holidays_locale'})\n",
    "train = train.merge(holidays, on='date', how='left', suffixes=('', '_holiday'))\n",
    "test = test.merge(holidays, on='date', how='left', suffixes=('', '_holiday'))\n",
    "\n",
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c647da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T09:33:46.798476Z",
     "iopub.status.busy": "2025-08-26T09:33:46.797742Z",
     "iopub.status.idle": "2025-08-26T09:33:48.714822Z",
     "shell.execute_reply": "2025-08-26T09:33:48.713682Z"
    },
    "papermill": {
     "duration": 1.921812,
     "end_time": "2025-08-26T09:33:48.716284",
     "exception": false,
     "start_time": "2025-08-26T09:33:46.794472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---missing value---\n",
      "id                       0\n",
      "date                     0\n",
      "store_nbr                0\n",
      "family                   0\n",
      "sales                    0\n",
      "onpromotion              0\n",
      "city                     0\n",
      "state                    0\n",
      "type                     0\n",
      "cluster                  0\n",
      "transactions        249117\n",
      "dcoilwtico          955152\n",
      "holidays_locale    2551824\n",
      "transferred        2551824\n",
      "dtype: int64\n",
      "-------------------\n",
      "id                     0\n",
      "date                   0\n",
      "store_nbr              0\n",
      "family                 0\n",
      "onpromotion            0\n",
      "city                   0\n",
      "state                  0\n",
      "type                   0\n",
      "cluster                0\n",
      "transactions       28512\n",
      "dcoilwtico          7128\n",
      "holidays_locale    26730\n",
      "transferred        26730\n",
      "dtype: int64\n",
      "\n",
      "---unique value---\n",
      "id:3000888\n",
      "date:1684\n",
      "store_nbr:54\n",
      "family:33\n",
      "sales:379610\n",
      "onpromotion:362\n",
      "city:22\n",
      "state:16\n",
      "type:5\n",
      "cluster:17\n",
      "transactions:4993\n",
      "dcoilwtico:994\n",
      "holidays_locale:3\n",
      "transferred:2\n"
     ]
    }
   ],
   "source": [
    "# check the missing values\n",
    "print(\"---missing value---\")\n",
    "print(train.isnull().sum())\n",
    "print(\"-------------------\")\n",
    "print(test.isnull().sum())\n",
    "print(\"\\n---unique value---\")\n",
    "for col in train:\n",
    "    print(f\"{col}:{train[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c897fd5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T09:33:48.724186Z",
     "iopub.status.busy": "2025-08-26T09:33:48.723867Z",
     "iopub.status.idle": "2025-08-26T09:33:50.703433Z",
     "shell.execute_reply": "2025-08-26T09:33:50.702328Z"
    },
    "papermill": {
     "duration": 1.985213,
     "end_time": "2025-08-26T09:33:50.704898",
     "exception": false,
     "start_time": "2025-08-26T09:33:48.719685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/1935353258.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['transferred'] = train['transferred'].fillna(False)\n",
      "/tmp/ipykernel_13/1935353258.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['transferred'] = test['transferred'].fillna(False)\n",
      "/tmp/ipykernel_13/1935353258.py:13: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train['dcoilwtico'] = train['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')  # linear interpolation\n",
      "/tmp/ipykernel_13/1935353258.py:15: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test['dcoilwtico'] = test['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---missing value---\n",
      "id                 0\n",
      "date               0\n",
      "store_nbr          0\n",
      "family             0\n",
      "sales              0\n",
      "onpromotion        0\n",
      "city               0\n",
      "state              0\n",
      "type               0\n",
      "cluster            0\n",
      "transactions       0\n",
      "dcoilwtico         0\n",
      "holidays_locale    0\n",
      "transferred        0\n",
      "dtype: int64\n",
      "-------------------\n",
      "id                 0\n",
      "date               0\n",
      "store_nbr          0\n",
      "family             0\n",
      "onpromotion        0\n",
      "city               0\n",
      "state              0\n",
      "type               0\n",
      "cluster            0\n",
      "transactions       0\n",
      "dcoilwtico         0\n",
      "holidays_locale    0\n",
      "transferred        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#fill in the missing value\n",
    "train['holidays_locale'] = train['holidays_locale'].fillna(False)\n",
    "test['holidays_locale'] = test['holidays_locale'].fillna(False)\n",
    "\n",
    "train['transferred'] = train['transferred'].fillna(False)\n",
    "test['transferred'] = test['transferred'].fillna(False)\n",
    "\n",
    "train['transactions'] = train.groupby('store_nbr')['transactions'].transform(lambda x: x.fillna(x.median()))\n",
    "store_medians = train.groupby('store_nbr')['transactions'].median()\n",
    "test['transactions'] = test['store_nbr'].map(store_medians)  # fill with median\n",
    "\n",
    "train['dcoilwtico'] = train['dcoilwtico'].interpolate(method='linear')\n",
    "train['dcoilwtico'] = train['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')  # linear interpolation\n",
    "test['dcoilwtico'] = test['dcoilwtico'].interpolate(method='linear')\n",
    "test['dcoilwtico'] = test['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# check the missing values\n",
    "print(\"---missing value---\")\n",
    "print(train.isnull().sum())\n",
    "print(\"-------------------\")\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c28e927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T09:33:50.712349Z",
     "iopub.status.busy": "2025-08-26T09:33:50.712029Z",
     "iopub.status.idle": "2025-08-26T09:33:51.642113Z",
     "shell.execute_reply": "2025-08-26T09:33:51.641377Z"
    },
    "papermill": {
     "duration": 0.93563,
     "end_time": "2025-08-26T09:33:51.643793",
     "exception": false,
     "start_time": "2025-08-26T09:33:50.708163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract temporal feature\n",
    "train['year'] = train['date'].dt.year\n",
    "train['month'] = train['date'].dt.month\n",
    "train['day'] = train['date'].dt.day\n",
    "train['dayofweek'] = train['date'].dt.dayofweek\n",
    "train['weekofyear'] = train['date'].dt.isocalendar().week.astype(int)\n",
    "train['is_weekend'] = train['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "test['year'] = test['date'].dt.year\n",
    "test['month'] = test['date'].dt.month\n",
    "test['day'] = test['date'].dt.day\n",
    "test['dayofweek'] = test['date'].dt.dayofweek\n",
    "test['weekofyear'] = test['date'].dt.isocalendar().week.astype(int)\n",
    "test['is_weekend'] = test['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "#adjust the order of dataset\n",
    "train_order = [\n",
    "    'id','date','year','month','day','dayofweek','weekofyear','is_weekend', 'holidays_locale', 'transferred', \n",
    "    'dcoilwtico',\n",
    "    'store_nbr', 'city', 'state', 'type', 'cluster',\n",
    "    'onpromotion', 'family', 'transactions','sales'\n",
    "]\n",
    "\n",
    "test_order = [\n",
    "    'id','date', 'year','month','day','dayofweek','weekofyear','is_weekend','holidays_locale', 'transferred', \n",
    "    'dcoilwtico',\n",
    "    'store_nbr',  'city', 'state', 'type', 'cluster',\n",
    "    'onpromotion', 'family', 'transactions'\n",
    "]\n",
    "\n",
    "train = train[train_order]\n",
    "test = test[test_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a75c63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T09:33:51.651705Z",
     "iopub.status.busy": "2025-08-26T09:33:51.651407Z",
     "iopub.status.idle": "2025-08-26T09:36:44.912538Z",
     "shell.execute_reply": "2025-08-26T09:36:44.911592Z"
    },
    "papermill": {
     "duration": 173.267029,
     "end_time": "2025-08-26T09:36:44.914129",
     "exception": false,
     "start_time": "2025-08-26T09:33:51.647100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's rmse: 322.05\tvalid's rmse: 353.775\n",
      "[200]\ttrain's rmse: 276.669\tvalid's rmse: 306.274\n",
      "[300]\ttrain's rmse: 253.4\tvalid's rmse: 293.078\n",
      "[400]\ttrain's rmse: 238.046\tvalid's rmse: 285.716\n",
      "[500]\ttrain's rmse: 228.349\tvalid's rmse: 281.64\n",
      "[600]\ttrain's rmse: 220.039\tvalid's rmse: 278.893\n",
      "[700]\ttrain's rmse: 213.177\tvalid's rmse: 278.422\n",
      "[800]\ttrain's rmse: 207.659\tvalid's rmse: 277.461\n",
      "[900]\ttrain's rmse: 202.4\tvalid's rmse: 275.972\n",
      "[1000]\ttrain's rmse: 197.577\tvalid's rmse: 276.572\n",
      "Early stopping, best iteration is:\n",
      "[953]\ttrain's rmse: 199.5\tvalid's rmse: 275.253\n",
      "Validation RMSE: 275.2531\n",
      "Validation RMSLE: 1.3153\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# split categorical and numerical columns\n",
    "cat_cols = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster', 'holidays_locale', 'transferred']\n",
    "encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_values = pd.concat([train[col], test[col]], axis=0).astype(str) # merge the collections to avoid potential errors\n",
    "    le.fit(all_values)\n",
    "    encoders[col] = le\n",
    "    train[col] = le.transform(train[col].astype(str))\n",
    "    test[col] = le.transform(test[col].astype(str))\n",
    "    \n",
    "# splite train set and validation set，dependent and independent variables\n",
    "X_train = train[train['date'] < '2017-03-01'].drop(columns=['sales', 'date', 'id'])\n",
    "y_train = train[train['date'] < '2017-03-01']['sales']\n",
    "X_val = train[train['date'] >= '2017-03-01'].drop(columns=['sales', 'date', 'id'])\n",
    "y_val = train[train['date'] >= '2017-03-01']['sales']\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 64,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# validation\n",
    "y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(y_val_pred, 0)))\n",
    "\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "print(f\"Validation RMSLE: {rmsle:.4f}\")\n",
    "\n",
    "# prediction\n",
    "X_test = test.drop(columns=['date','id'])\n",
    "y_test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "# generate submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'sales': np.maximum(y_test_pred, 0) \n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 192.663335,
   "end_time": "2025-08-26T09:36:45.738094",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-26T09:33:33.074759",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
