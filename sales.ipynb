{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# input dataset\ntrain = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\nstores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\nholidays = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv', parse_dates=['date'])\noil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv', parse_dates=['date'])\ntransactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv', parse_dates=['date'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# adjust date formats\ntrain['date'] = pd.to_datetime(train['date'])\ntest['date'] = pd.to_datetime(test['date'])\ntransactions['date'] = pd.to_datetime(transactions['date'])\nholidays['date'] = pd.to_datetime(holidays['date'])\noil['date'] = pd.to_datetime(oil['date'])\n\n# merge the dataset\ntrain = train.merge(stores, on='store_nbr', how='left')\ntest = test.merge(stores, on='store_nbr', how='left')\n\ntrain = train.merge(transactions, on=['date', 'store_nbr'], how='left')\ntest = test.merge(transactions, on=['date', 'store_nbr'], how='left')\n\ntrain = train.merge(oil, on='date', how='left')\ntest = test.merge(oil, on='date', how='left')\n\nholidays = holidays[['date', 'locale', 'transferred']].rename(columns={'locale': 'holidays_locale'})\ntrain = train.merge(holidays, on='date', how='left', suffixes=('', '_holiday'))\ntest = test.merge(holidays, on='date', how='left', suffixes=('', '_holiday'))\n\ntrain.info()\ntest.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check the missing values\nprint(\"---missing value---\")\nprint(train.isnull().sum())\n\nprint(\"\\n---unique value---\")\nfor col in train:\n    print(f\"{col}:{train[col].nunique()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#fill in the missing value\ntrain['holidays_locale'] = train['holidays_locale'].fillna(False)\ntrain['transferred'] = train['transferred'].fillna(False)\ntrain['transactions'] = train.groupby('store_nbr')['transactions'].transform(lambda x: x.fillna(x.median()))\ntest['transactions'] = test.groupby('store_nbr')['transactions'].transform(lambda x: x.fillna(x.median()))\ntrain['dcoilwtico'] = train['dcoilwtico'].interpolate(method='linear')\ntrain['dcoilwtico'] = train['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check the missing values\nprint(\"---missing value---\")\nprint(train.isnull().sum())\nprint(test.isnull().sum())\n\nprint(\"\\n---unique value---\")\nfor col in train:\n    print(f\"{col}:{train[col].nunique()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#extract temporal feature\ntrain['year'] = train['date'].dt.year\ntrain['month'] = train['date'].dt.month\ntrain['day'] = train['date'].dt.day\ntrain['dayofweek'] = train['date'].dt.dayofweek\ntrain['weekofyear'] = train['date'].dt.isocalendar().week.astype(int)\ntrain['is_weekend'] = train['dayofweek'].isin([5, 6]).astype(int)\n\ntest['year'] = test['date'].dt.year\ntest['month'] = test['date'].dt.month\ntest['day'] = test['date'].dt.day\ntest['dayofweek'] = test['date'].dt.dayofweek\ntest['weekofyear'] = test['date'].dt.isocalendar().week.astype(int)\ntest['is_weekend'] = test['dayofweek'].isin([5, 6]).astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#adjust the order of dataset\ntrain_order = [\n    'id', 'date','year','month','day','dayofweek','weekofyear','is_weekend', 'holidays_locale', 'transferred', \n    'dcoilwtico',\n    'store_nbr', 'city', 'state', 'type', 'cluster',\n    'onpromotion', 'family', 'transactions','sales'\n]\n\ntest_order = [\n    'id', 'date','year','month','day','dayofweek','weekofyear','is_weekend','holidays_locale', 'transferred', \n    'dcoilwtico',\n    'store_nbr',  'city', 'state', 'type', 'cluster',\n   'onpromotion', 'family'\n]\n\ntrain = train[train_order]\ntest = test[test_order]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nimport lightgbm as lgb\n\n# splite train set and validation set，dependent and independent variables\nX_train = train[train['date'] < '2017-01-01'].drop(columns=['sales', 'date', 'id'])\ny_train = train[train['date'] < '2017-01-01']['sales']\nX_val = train[train['date'] >= '2017-01-01'].drop(columns=['sales', 'date', 'id'])\ny_val = train[train['date'] >= '2017-01-01']['sales']\n\ntest = test.drop(columns=['id', 'date'])\n\n# split categorical and numerical columns\ncat_cols = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster', 'holidays_locale', 'transferred']\nencoders = {}\n\nfor col in cat_cols:\n    le = LabelEncoder()\n    all_values = pd.concat([train[col], test[col]], axis=0).astype(str)\n    le.fit(all_values)\n    encoders[col] = le\n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nval_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\nX_train.info()\nX_val.info()\ntest.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'num_leaves': 64,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': -1\n}\n\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=2000,\n    valid_sets=[train_data, val_data],\n    valid_names=['train', 'valid'],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=100),\n        lgb.log_evaluation(period=100)\n    ]\n)\n\n# 验证集预测\ny_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n\nrmse = mean_squared_error(y_val, y_val_pred, squared=False)\nrmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(y_val_pred, 0)))\n\nprint(f\"Validation RMSE: {rmse:.4f}\")\nprint(f\"Validation RMSLE: {rmsle:.4f}\")\n\n# 在 test 上预测\nX_test = test.drop(columns=['date','id'])\ny_test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n# 生成提交文件\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'sales': np.maximum(y_test_pred, 0)   # 销量不能为负\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"提交文件已生成 submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}