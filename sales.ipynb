{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-25T09:25:39.535756Z","iopub.execute_input":"2025-08-25T09:25:39.536081Z","iopub.status.idle":"2025-08-25T09:25:39.552887Z","shell.execute_reply.started":"2025-08-25T09:25:39.536060Z","shell.execute_reply":"2025-08-25T09:25:39.551797Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/store-sales-time-series-forecasting/oil.csv\n/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv\n/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\n/kaggle/input/store-sales-time-series-forecasting/stores.csv\n/kaggle/input/store-sales-time-series-forecasting/train.csv\n/kaggle/input/store-sales-time-series-forecasting/test.csv\n/kaggle/input/store-sales-time-series-forecasting/transactions.csv\n","output_type":"stream"}],"execution_count":190},{"cell_type":"code","source":"# input dataset\ntrain = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\nstores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\nholidays = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv', parse_dates=['date'])\noil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv', parse_dates=['date'])\ntransactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv', parse_dates=['date'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T09:25:39.554739Z","iopub.execute_input":"2025-08-25T09:25:39.555120Z","iopub.status.idle":"2025-08-25T09:25:41.470680Z","shell.execute_reply.started":"2025-08-25T09:25:39.555088Z","shell.execute_reply":"2025-08-25T09:25:41.469838Z"}},"outputs":[],"execution_count":191},{"cell_type":"code","source":"# adjust date formats\ntrain['date'] = pd.to_datetime(train['date'])\ntest['date'] = pd.to_datetime(test['date'])\ntransactions['date'] = pd.to_datetime(transactions['date'])\nholidays['date'] = pd.to_datetime(holidays['date'])\noil['date'] = pd.to_datetime(oil['date'])\n\n# merge the dataset\ntrain = train.merge(stores, on='store_nbr', how='left')\ntest = test.merge(stores, on='store_nbr', how='left')\n\ntrain = train.merge(transactions, on=['date', 'store_nbr'], how='left')\ntest = test.merge(transactions, on=['date', 'store_nbr'], how='left')\n\ntrain = train.merge(oil, on='date', how='left')\ntest = test.merge(oil, on='date', how='left')\n\nholidays = holidays[['date', 'locale', 'transferred']].rename(columns={'locale': 'holidays_locale'})\ntrain = train.merge(holidays, on='date', how='left', suffixes=('', '_holiday'))\ntest = test.merge(holidays, on='date', how='left', suffixes=('', '_holiday'))\n\ntrain.info()\ntest.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T09:25:41.471548Z","iopub.execute_input":"2025-08-25T09:25:41.471944Z","iopub.status.idle":"2025-08-25T09:25:43.748678Z","shell.execute_reply.started":"2025-08-25T09:25:41.471912Z","shell.execute_reply":"2025-08-25T09:25:43.747624Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3054348 entries, 0 to 3054347\nData columns (total 14 columns):\n #   Column           Dtype         \n---  ------           -----         \n 0   id               int64         \n 1   date             datetime64[ns]\n 2   store_nbr        int64         \n 3   family           object        \n 4   sales            float64       \n 5   onpromotion      int64         \n 6   city             object        \n 7   state            object        \n 8   type             object        \n 9   cluster          int64         \n 10  transactions     float64       \n 11  dcoilwtico       float64       \n 12  holidays_locale  object        \n 13  transferred      object        \ndtypes: datetime64[ns](1), float64(3), int64(4), object(6)\nmemory usage: 326.2+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 28512 entries, 0 to 28511\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype         \n---  ------           --------------  -----         \n 0   id               28512 non-null  int64         \n 1   date             28512 non-null  datetime64[ns]\n 2   store_nbr        28512 non-null  int64         \n 3   family           28512 non-null  object        \n 4   onpromotion      28512 non-null  int64         \n 5   city             28512 non-null  object        \n 6   state            28512 non-null  object        \n 7   type             28512 non-null  object        \n 8   cluster          28512 non-null  int64         \n 9   transactions     0 non-null      float64       \n 10  dcoilwtico       21384 non-null  float64       \n 11  holidays_locale  1782 non-null   object        \n 12  transferred      1782 non-null   object        \ndtypes: datetime64[ns](1), float64(2), int64(4), object(6)\nmemory usage: 2.8+ MB\n","output_type":"stream"}],"execution_count":192},{"cell_type":"code","source":"# check the missing values\nprint(\"---missing value---\")\nprint(train.isnull().sum())\nprint(test.isnull().sum())\n\nprint(\"\\n---unique value---\")\nfor col in train:\n    print(f\"{col}:{train[col].nunique()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T09:25:43.749601Z","iopub.execute_input":"2025-08-25T09:25:43.749923Z","iopub.status.idle":"2025-08-25T09:25:45.716477Z","shell.execute_reply.started":"2025-08-25T09:25:43.749902Z","shell.execute_reply":"2025-08-25T09:25:45.715706Z"}},"outputs":[{"name":"stdout","text":"---missing value---\nid                       0\ndate                     0\nstore_nbr                0\nfamily                   0\nsales                    0\nonpromotion              0\ncity                     0\nstate                    0\ntype                     0\ncluster                  0\ntransactions        249117\ndcoilwtico          955152\nholidays_locale    2551824\ntransferred        2551824\ndtype: int64\nid                     0\ndate                   0\nstore_nbr              0\nfamily                 0\nonpromotion            0\ncity                   0\nstate                  0\ntype                   0\ncluster                0\ntransactions       28512\ndcoilwtico          7128\nholidays_locale    26730\ntransferred        26730\ndtype: int64\n\n---unique value---\nid:3000888\ndate:1684\nstore_nbr:54\nfamily:33\nsales:379610\nonpromotion:362\ncity:22\nstate:16\ntype:5\ncluster:17\ntransactions:4993\ndcoilwtico:994\nholidays_locale:3\ntransferred:2\n","output_type":"stream"}],"execution_count":193},{"cell_type":"code","source":"#fill in the missing value\ntrain['holidays_locale'] = train['holidays_locale'].fillna(False)\ntest['holidays_locale'] = test['holidays_locale'].fillna(False)\n\ntrain['transferred'] = train['transferred'].fillna(False)\ntest['transferred'] = test['transferred'].fillna(False)\n\ntrain['transactions'] = train.groupby('store_nbr')['transactions'].transform(lambda x: x.fillna(x.median()))\nstore_medians = train.groupby('store_nbr')['transactions'].median()\ntest['transactions'] = test['store_nbr'].map(store_medians)\n\ntrain['dcoilwtico'] = train['dcoilwtico'].interpolate(method='linear')\ntest['dcoilwtico'] = test['dcoilwtico'].interpolate(method='linear')\ntrain['dcoilwtico'] = train['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')\ntest['dcoilwtico'] = test['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T09:25:45.718873Z","iopub.execute_input":"2025-08-25T09:25:45.719463Z","iopub.status.idle":"2025-08-25T09:25:46.975291Z","shell.execute_reply.started":"2025-08-25T09:25:45.719439Z","shell.execute_reply":"2025-08-25T09:25:46.974366Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2010465726.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train['transferred'] = train['transferred'].fillna(False)\n/tmp/ipykernel_36/2010465726.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  test['transferred'] = test['transferred'].fillna(False)\n/tmp/ipykernel_36/2010465726.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  train['dcoilwtico'] = train['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')\n/tmp/ipykernel_36/2010465726.py:15: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  test['dcoilwtico'] = test['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')\n","output_type":"stream"}],"execution_count":194},{"cell_type":"code","source":"# check the missing values\nprint(\"---missing value---\")\nprint(train.isnull().sum())\nprint(test.isnull().sum())\n\nprint(\"\\n---unique value---\")\nfor col in train:\n    print(f\"{col}:{train[col].nunique()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T09:25:46.976247Z","iopub.execute_input":"2025-08-25T09:25:46.976548Z","iopub.status.idle":"2025-08-25T09:25:48.815296Z","shell.execute_reply.started":"2025-08-25T09:25:46.976526Z","shell.execute_reply":"2025-08-25T09:25:48.814304Z"}},"outputs":[{"name":"stdout","text":"---missing value---\nid                 0\ndate               0\nstore_nbr          0\nfamily             0\nsales              0\nonpromotion        0\ncity               0\nstate              0\ntype               0\ncluster            0\ntransactions       0\ndcoilwtico         0\nholidays_locale    0\ntransferred        0\ndtype: int64\nid                 0\ndate               0\nstore_nbr          0\nfamily             0\nonpromotion        0\ncity               0\nstate              0\ntype               0\ncluster            0\ntransactions       0\ndcoilwtico         0\nholidays_locale    0\ntransferred        0\ndtype: int64\n\n---unique value---\nid:3000888\ndate:1684\nstore_nbr:54\nfamily:33\nsales:379610\nonpromotion:362\ncity:22\nstate:16\ntype:5\ncluster:17\ntransactions:5001\ndcoilwtico:933305\nholidays_locale:4\ntransferred:2\n","output_type":"stream"}],"execution_count":195},{"cell_type":"code","source":"#extract temporal feature\ntrain['year'] = train['date'].dt.year\ntrain['month'] = train['date'].dt.month\ntrain['day'] = train['date'].dt.day\ntrain['dayofweek'] = train['date'].dt.dayofweek\ntrain['weekofyear'] = train['date'].dt.isocalendar().week.astype(int)\ntrain['is_weekend'] = train['dayofweek'].isin([5, 6]).astype(int)\n\ntest['year'] = test['date'].dt.year\ntest['month'] = test['date'].dt.month\ntest['day'] = test['date'].dt.day\ntest['dayofweek'] = test['date'].dt.dayofweek\ntest['weekofyear'] = test['date'].dt.isocalendar().week.astype(int)\ntest['is_weekend'] = test['dayofweek'].isin([5, 6]).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T09:25:48.816286Z","iopub.execute_input":"2025-08-25T09:25:48.816598Z","iopub.status.idle":"2025-08-25T09:25:49.346658Z","shell.execute_reply.started":"2025-08-25T09:25:48.816564Z","shell.execute_reply":"2025-08-25T09:25:49.345894Z"}},"outputs":[],"execution_count":196},{"cell_type":"code","source":"#adjust the order of dataset\ntrain_order = [\n    'id', 'date','year','month','day','dayofweek','weekofyear','is_weekend', 'holidays_locale', 'transferred', \n    'dcoilwtico',\n    'store_nbr', 'city', 'state', 'type', 'cluster',\n    'onpromotion', 'family', 'transactions','sales'\n]\n\ntest_order = [\n    'id', 'date','year','month','day','dayofweek','weekofyear','is_weekend','holidays_locale', 'transferred', \n    'dcoilwtico',\n    'store_nbr',  'city', 'state', 'type', 'cluster',\n   'onpromotion', 'family', 'transactions'\n]\n\ntrain = train[train_order]\ntest = test[test_order]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T09:25:49.347577Z","iopub.execute_input":"2025-08-25T09:25:49.347832Z","iopub.status.idle":"2025-08-25T09:25:49.612135Z","shell.execute_reply.started":"2025-08-25T09:25:49.347813Z","shell.execute_reply":"2025-08-25T09:25:49.611209Z"}},"outputs":[],"execution_count":197},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nimport lightgbm as lgb\n\n# split categorical and numerical columns\ncat_cols = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster', 'holidays_locale', 'transferred']\nencoders = {}\n\nfor col in cat_cols:\n    le = LabelEncoder()\n    all_values = pd.concat([train[col], test[col]], axis=0).astype(str)\n    le.fit(all_values)\n    encoders[col] = le\n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n    \n# splite train set and validation set，dependent and independent variables\nX_train = train[train['date'] < '2017-01-01'].drop(columns=['sales', 'date', 'id'])\ny_train = train[train['date'] < '2017-01-01']['sales']\nX_val = train[train['date'] >= '2017-01-01'].drop(columns=['sales', 'date', 'id'])\ny_val = train[train['date'] >= '2017-01-01']['sales']\n\ntest = test.drop(columns=['id', 'date'])\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nval_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\nX_train.info()\nX_val.info()\ntest.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T09:25:49.613176Z","iopub.execute_input":"2025-08-25T09:25:49.613455Z","iopub.status.idle":"2025-08-25T09:26:02.188445Z","shell.execute_reply.started":"2025-08-25T09:25:49.613434Z","shell.execute_reply":"2025-08-25T09:26:02.187581Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 2642706 entries, 0 to 2642705\nData columns (total 17 columns):\n #   Column           Dtype  \n---  ------           -----  \n 0   year             int32  \n 1   month            int32  \n 2   day              int32  \n 3   dayofweek        int32  \n 4   weekofyear       int64  \n 5   is_weekend       int64  \n 6   holidays_locale  int64  \n 7   transferred      int64  \n 8   dcoilwtico       float64\n 9   store_nbr        int64  \n 10  city             int64  \n 11  state            int64  \n 12  type             int64  \n 13  cluster          int64  \n 14  onpromotion      int64  \n 15  family           int64  \n 16  transactions     float64\ndtypes: float64(2), int32(4), int64(11)\nmemory usage: 322.6 MB\n<class 'pandas.core.frame.DataFrame'>\nIndex: 411642 entries, 2642706 to 3054347\nData columns (total 17 columns):\n #   Column           Non-Null Count   Dtype  \n---  ------           --------------   -----  \n 0   year             411642 non-null  int32  \n 1   month            411642 non-null  int32  \n 2   day              411642 non-null  int32  \n 3   dayofweek        411642 non-null  int32  \n 4   weekofyear       411642 non-null  int64  \n 5   is_weekend       411642 non-null  int64  \n 6   holidays_locale  411642 non-null  int64  \n 7   transferred      411642 non-null  int64  \n 8   dcoilwtico       411642 non-null  float64\n 9   store_nbr        411642 non-null  int64  \n 10  city             411642 non-null  int64  \n 11  state            411642 non-null  int64  \n 12  type             411642 non-null  int64  \n 13  cluster          411642 non-null  int64  \n 14  onpromotion      411642 non-null  int64  \n 15  family           411642 non-null  int64  \n 16  transactions     411642 non-null  float64\ndtypes: float64(2), int32(4), int64(11)\nmemory usage: 50.2 MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 28512 entries, 0 to 28511\nData columns (total 17 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   year             28512 non-null  int32  \n 1   month            28512 non-null  int32  \n 2   day              28512 non-null  int32  \n 3   dayofweek        28512 non-null  int32  \n 4   weekofyear       28512 non-null  int64  \n 5   is_weekend       28512 non-null  int64  \n 6   holidays_locale  28512 non-null  int64  \n 7   transferred      28512 non-null  int64  \n 8   dcoilwtico       28512 non-null  float64\n 9   store_nbr        28512 non-null  int64  \n 10  city             28512 non-null  int64  \n 11  state            28512 non-null  int64  \n 12  type             28512 non-null  int64  \n 13  cluster          28512 non-null  int64  \n 14  onpromotion      28512 non-null  int64  \n 15  family           28512 non-null  int64  \n 16  transactions     28512 non-null  float64\ndtypes: float64(2), int32(4), int64(11)\nmemory usage: 3.3 MB\n","output_type":"stream"}],"execution_count":198},{"cell_type":"code","source":"params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'num_leaves': 64,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': -1\n}\n\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=2000,\n    valid_sets=[train_data, val_data],\n    valid_names=['train', 'valid'],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=100),\n        lgb.log_evaluation(period=100)\n    ]\n)\n\n# 验证集预测\ny_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n\nrmse = mean_squared_error(y_val, y_val_pred, squared=False)\nrmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(y_val_pred, 0)))\n\nprint(f\"Validation RMSE: {rmse:.4f}\")\nprint(f\"Validation RMSLE: {rmsle:.4f}\")\n\n# 在 test 上预测\n#X_test = test.drop(columns=['date','id'])\ny_test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n# 生成提交文件\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'sales': np.maximum(y_test_pred, 0)   # 销量不能为负\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"提交文件已生成 submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T09:26:02.189484Z","iopub.execute_input":"2025-08-25T09:26:02.189816Z","iopub.status.idle":"2025-08-25T09:27:45.829216Z","shell.execute_reply.started":"2025-08-25T09:26:02.189787Z","shell.execute_reply":"2025-08-25T09:27:45.827748Z"}},"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 100 rounds\n[100]\ttrain's rmse: 321.671\tvalid's rmse: 375.773\n[200]\ttrain's rmse: 273.641\tvalid's rmse: 335.942\n[300]\ttrain's rmse: 250.873\tvalid's rmse: 325.056\n[400]\ttrain's rmse: 236.794\tvalid's rmse: 319.129\n[500]\ttrain's rmse: 226.507\tvalid's rmse: 318.81\n[600]\ttrain's rmse: 218.462\tvalid's rmse: 317.416\n[700]\ttrain's rmse: 211.347\tvalid's rmse: 316.762\n[800]\ttrain's rmse: 205.863\tvalid's rmse: 317.114\nEarly stopping, best iteration is:\n[705]\ttrain's rmse: 211.039\tvalid's rmse: 316.6\nValidation RMSE: 316.5996\nValidation RMSLE: 1.4080\n","output_type":"stream"},{"name":"stderr","text":"[LightGBM] [Fatal] The number of features in data (10) is not the same as it was in training data (17).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3374685080.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# 在 test 上预测\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#X_test = test.drop(columns=['date','id'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# 生成提交文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   4746\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4747\u001b[0m                 \u001b[0mnum_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4748\u001b[0;31m         return predictor.predict(\n\u001b[0m\u001b[1;32m   4749\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4750\u001b[0m             \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             )\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             preds, nrow = self.__pred_for_np2d(\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0mmat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m             return self.__inner_predict_np2d(\n\u001b[0m\u001b[1;32m   1345\u001b[0m                 \u001b[0mmat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_predict_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong length of pre-allocated predict array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mout_num_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         _safe_call(\n\u001b[0m\u001b[1;32m   1291\u001b[0m             _LIB.LGBM_BoosterPredictForMat(\n\u001b[1;32m   1292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \"\"\"\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLightGBMError\u001b[0m: The number of features in data (10) is not the same as it was in training data (17).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing."],"ename":"LightGBMError","evalue":"The number of features in data (10) is not the same as it was in training data (17).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.","output_type":"error"}],"execution_count":199}]}