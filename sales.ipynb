{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2193e51c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-29T02:18:29.620645Z",
     "iopub.status.busy": "2025-08-29T02:18:29.620365Z",
     "iopub.status.idle": "2025-08-29T02:18:31.413704Z",
     "shell.execute_reply": "2025-08-29T02:18:31.412573Z"
    },
    "papermill": {
     "duration": 1.798962,
     "end_time": "2025-08-29T02:18:31.415381",
     "exception": false,
     "start_time": "2025-08-29T02:18:29.616419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/store-sales-time-series-forecasting/oil.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/stores.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/train.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/test.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/transactions.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc807549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T02:18:31.422050Z",
     "iopub.status.busy": "2025-08-29T02:18:31.421675Z",
     "iopub.status.idle": "2025-08-29T02:18:34.299859Z",
     "shell.execute_reply": "2025-08-29T02:18:34.299054Z"
    },
    "papermill": {
     "duration": 2.883009,
     "end_time": "2025-08-29T02:18:34.301565",
     "exception": false,
     "start_time": "2025-08-29T02:18:31.418556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input dataset\n",
    "train = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\n",
    "stores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\n",
    "holidays = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv', parse_dates=['date'])\n",
    "oil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv', parse_dates=['date'])\n",
    "transactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d10c452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T02:18:34.307741Z",
     "iopub.status.busy": "2025-08-29T02:18:34.307418Z",
     "iopub.status.idle": "2025-08-29T02:18:37.039612Z",
     "shell.execute_reply": "2025-08-29T02:18:37.038637Z"
    },
    "papermill": {
     "duration": 2.737084,
     "end_time": "2025-08-29T02:18:37.041129",
     "exception": false,
     "start_time": "2025-08-29T02:18:34.304045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3054348 entries, 0 to 3054347\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   id               int64         \n",
      " 1   date             datetime64[ns]\n",
      " 2   store_nbr        int64         \n",
      " 3   family           object        \n",
      " 4   sales            float64       \n",
      " 5   onpromotion      int64         \n",
      " 6   city             object        \n",
      " 7   state            object        \n",
      " 8   type             object        \n",
      " 9   cluster          int64         \n",
      " 10  transactions     float64       \n",
      " 11  dcoilwtico       float64       \n",
      " 12  holidays_locale  object        \n",
      " 13  transferred      object        \n",
      "dtypes: datetime64[ns](1), float64(3), int64(4), object(6)\n",
      "memory usage: 326.2+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28512 entries, 0 to 28511\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   id               28512 non-null  int64         \n",
      " 1   date             28512 non-null  datetime64[ns]\n",
      " 2   store_nbr        28512 non-null  int64         \n",
      " 3   family           28512 non-null  object        \n",
      " 4   onpromotion      28512 non-null  int64         \n",
      " 5   city             28512 non-null  object        \n",
      " 6   state            28512 non-null  object        \n",
      " 7   type             28512 non-null  object        \n",
      " 8   cluster          28512 non-null  int64         \n",
      " 9   transactions     0 non-null      float64       \n",
      " 10  dcoilwtico       21384 non-null  float64       \n",
      " 11  holidays_locale  1782 non-null   object        \n",
      " 12  transferred      1782 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(4), object(6)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# adjust date formats\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "transactions['date'] = pd.to_datetime(transactions['date'])\n",
    "holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "oil['date'] = pd.to_datetime(oil['date'])\n",
    "\n",
    "# merge the dataset\n",
    "train = train.merge(stores, on='store_nbr', how='left')\n",
    "test = test.merge(stores, on='store_nbr', how='left')\n",
    "\n",
    "train = train.merge(transactions, on=['date', 'store_nbr'], how='left')\n",
    "test = test.merge(transactions, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "train = train.merge(oil, on='date', how='left')\n",
    "test = test.merge(oil, on='date', how='left')\n",
    "\n",
    "holidays = holidays[['date', 'locale', 'transferred']].rename(columns={'locale': 'holidays_locale'})\n",
    "train = train.merge(holidays, on='date', how='left', suffixes=('', '_holiday'))\n",
    "test = test.merge(holidays, on='date', how='left', suffixes=('', '_holiday'))\n",
    "\n",
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027b0804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T02:18:37.047392Z",
     "iopub.status.busy": "2025-08-29T02:18:37.047107Z",
     "iopub.status.idle": "2025-08-29T02:18:38.565981Z",
     "shell.execute_reply": "2025-08-29T02:18:38.564832Z"
    },
    "papermill": {
     "duration": 1.523857,
     "end_time": "2025-08-29T02:18:38.567544",
     "exception": false,
     "start_time": "2025-08-29T02:18:37.043687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---missing value---\n",
      "id                       0\n",
      "date                     0\n",
      "store_nbr                0\n",
      "family                   0\n",
      "sales                    0\n",
      "onpromotion              0\n",
      "city                     0\n",
      "state                    0\n",
      "type                     0\n",
      "cluster                  0\n",
      "transactions        249117\n",
      "dcoilwtico          955152\n",
      "holidays_locale    2551824\n",
      "transferred        2551824\n",
      "dtype: int64\n",
      "-------------------\n",
      "id                     0\n",
      "date                   0\n",
      "store_nbr              0\n",
      "family                 0\n",
      "onpromotion            0\n",
      "city                   0\n",
      "state                  0\n",
      "type                   0\n",
      "cluster                0\n",
      "transactions       28512\n",
      "dcoilwtico          7128\n",
      "holidays_locale    26730\n",
      "transferred        26730\n",
      "dtype: int64\n",
      "\n",
      "---unique value---\n",
      "id:3000888\n",
      "date:1684\n",
      "store_nbr:54\n",
      "family:33\n",
      "sales:379610\n",
      "onpromotion:362\n",
      "city:22\n",
      "state:16\n",
      "type:5\n",
      "cluster:17\n",
      "transactions:4993\n",
      "dcoilwtico:994\n",
      "holidays_locale:3\n",
      "transferred:2\n"
     ]
    }
   ],
   "source": [
    "# check the missing values\n",
    "print(\"---missing value---\")\n",
    "print(train.isnull().sum())\n",
    "print(\"-------------------\")\n",
    "print(test.isnull().sum())\n",
    "print(\"\\n---unique value---\")\n",
    "for col in train:\n",
    "    print(f\"{col}:{train[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6d819f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T02:18:38.574319Z",
     "iopub.status.busy": "2025-08-29T02:18:38.574021Z",
     "iopub.status.idle": "2025-08-29T02:18:40.300971Z",
     "shell.execute_reply": "2025-08-29T02:18:40.299482Z"
    },
    "papermill": {
     "duration": 1.732476,
     "end_time": "2025-08-29T02:18:40.302830",
     "exception": false,
     "start_time": "2025-08-29T02:18:38.570354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/1935353258.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['transferred'] = train['transferred'].fillna(False)\n",
      "/tmp/ipykernel_13/1935353258.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['transferred'] = test['transferred'].fillna(False)\n",
      "/tmp/ipykernel_13/1935353258.py:13: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train['dcoilwtico'] = train['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')  # linear interpolation\n",
      "/tmp/ipykernel_13/1935353258.py:15: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test['dcoilwtico'] = test['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---missing value---\n",
      "id                 0\n",
      "date               0\n",
      "store_nbr          0\n",
      "family             0\n",
      "sales              0\n",
      "onpromotion        0\n",
      "city               0\n",
      "state              0\n",
      "type               0\n",
      "cluster            0\n",
      "transactions       0\n",
      "dcoilwtico         0\n",
      "holidays_locale    0\n",
      "transferred        0\n",
      "dtype: int64\n",
      "-------------------\n",
      "id                 0\n",
      "date               0\n",
      "store_nbr          0\n",
      "family             0\n",
      "onpromotion        0\n",
      "city               0\n",
      "state              0\n",
      "type               0\n",
      "cluster            0\n",
      "transactions       0\n",
      "dcoilwtico         0\n",
      "holidays_locale    0\n",
      "transferred        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#fill in the missing value\n",
    "train['holidays_locale'] = train['holidays_locale'].fillna(False)\n",
    "test['holidays_locale'] = test['holidays_locale'].fillna(False)\n",
    "\n",
    "train['transferred'] = train['transferred'].fillna(False)\n",
    "test['transferred'] = test['transferred'].fillna(False)\n",
    "\n",
    "train['transactions'] = train.groupby('store_nbr')['transactions'].transform(lambda x: x.fillna(x.median()))\n",
    "store_medians = train.groupby('store_nbr')['transactions'].median()\n",
    "test['transactions'] = test['store_nbr'].map(store_medians)  # fill with median\n",
    "\n",
    "train['dcoilwtico'] = train['dcoilwtico'].interpolate(method='linear')\n",
    "train['dcoilwtico'] = train['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')  # linear interpolation\n",
    "test['dcoilwtico'] = test['dcoilwtico'].interpolate(method='linear')\n",
    "test['dcoilwtico'] = test['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# check the missing values\n",
    "print(\"---missing value---\")\n",
    "print(train.isnull().sum())\n",
    "print(\"-------------------\")\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15eb1ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T02:18:40.309943Z",
     "iopub.status.busy": "2025-08-29T02:18:40.309662Z",
     "iopub.status.idle": "2025-08-29T02:18:41.079315Z",
     "shell.execute_reply": "2025-08-29T02:18:41.078293Z"
    },
    "papermill": {
     "duration": 0.775102,
     "end_time": "2025-08-29T02:18:41.080928",
     "exception": false,
     "start_time": "2025-08-29T02:18:40.305826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract temporal feature\n",
    "train['year'] = train['date'].dt.year\n",
    "train['month'] = train['date'].dt.month\n",
    "train['day'] = train['date'].dt.day\n",
    "train['dayofweek'] = train['date'].dt.dayofweek\n",
    "train['weekofyear'] = train['date'].dt.isocalendar().week.astype(int)\n",
    "train['is_weekend'] = train['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "test['year'] = test['date'].dt.year\n",
    "test['month'] = test['date'].dt.month\n",
    "test['day'] = test['date'].dt.day\n",
    "test['dayofweek'] = test['date'].dt.dayofweek\n",
    "test['weekofyear'] = test['date'].dt.isocalendar().week.astype(int)\n",
    "test['is_weekend'] = test['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "#adjust the order of dataset\n",
    "train_order = [\n",
    "    'id','date','year','month','day','dayofweek','weekofyear','is_weekend', 'holidays_locale', 'transferred', \n",
    "    'dcoilwtico',\n",
    "    'store_nbr', 'city', 'state', 'type', 'cluster',\n",
    "    'onpromotion', 'family', 'transactions','sales'\n",
    "]\n",
    "\n",
    "test_order = [\n",
    "    'id','date', 'year','month','day','dayofweek','weekofyear','is_weekend','holidays_locale', 'transferred', \n",
    "    'dcoilwtico',\n",
    "    'store_nbr',  'city', 'state', 'type', 'cluster',\n",
    "    'onpromotion', 'family', 'transactions'\n",
    "]\n",
    "\n",
    "train = train[train_order]\n",
    "test = test[test_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d314f86e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T02:18:41.087749Z",
     "iopub.status.busy": "2025-08-29T02:18:41.087505Z",
     "iopub.status.idle": "2025-08-29T02:21:54.139128Z",
     "shell.execute_reply": "2025-08-29T02:21:54.138051Z"
    },
    "papermill": {
     "duration": 193.059411,
     "end_time": "2025-08-29T02:21:54.143333",
     "exception": false,
     "start_time": "2025-08-29T02:18:41.083922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's rmse: 0.559068\tvalid's rmse: 0.570537\n",
      "[200]\ttrain's rmse: 0.498857\tvalid's rmse: 0.533271\n",
      "[300]\ttrain's rmse: 0.469218\tvalid's rmse: 0.516598\n",
      "[400]\ttrain's rmse: 0.446823\tvalid's rmse: 0.505039\n",
      "[500]\ttrain's rmse: 0.431145\tvalid's rmse: 0.500192\n",
      "[600]\ttrain's rmse: 0.420233\tvalid's rmse: 0.497347\n",
      "[700]\ttrain's rmse: 0.409951\tvalid's rmse: 0.494786\n",
      "[800]\ttrain's rmse: 0.401599\tvalid's rmse: 0.493369\n",
      "[900]\ttrain's rmse: 0.394165\tvalid's rmse: 0.492471\n",
      "[1000]\ttrain's rmse: 0.388572\tvalid's rmse: 0.490765\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\ttrain's rmse: 0.388671\tvalid's rmse: 0.490761\n",
      "Validation RMSE: 1338.1138\n",
      "Validation RMSLE: 0.4900\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# numerical conversion\n",
    "cat_cols = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster', 'holidays_locale', 'transferred']\n",
    "encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_values = pd.concat([train[col], test[col]], axis=0).astype(str) # merge the collections to avoid potential errors\n",
    "    le.fit(all_values)\n",
    "    encoders[col] = le\n",
    "    train[col] = le.transform(train[col].astype(str))\n",
    "    test[col] = le.transform(test[col].astype(str))\n",
    "\n",
    "# logarithm transformation\n",
    "train['sales_log'] = np.log1p(train['sales'])\n",
    "\n",
    "# splite train set and validation set，dependent and independent variables\n",
    "X_train = train[train['date'] < '2017-03-01'].drop(columns=['sales','sales_log','date','id'])\n",
    "y_train = train[train['date'] < '2017-03-01']['sales_log']\n",
    "X_val   = train[train['date'] >= '2017-03-01'].drop(columns=['sales','sales_log','date','id'])\n",
    "y_val   = train[train['date'] >= '2017-03-01']['sales_log']\n",
    "\n",
    "# build the LightGBM collection\n",
    "train_data = lgb.Dataset(X_train, label = y_train)\n",
    "val_data = lgb.Dataset(X_val, label = y_val, reference = train_data)\n",
    "\n",
    "#set up the parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 128,\n",
    "    'max_depth': -1,      \n",
    "    'min_data_in_leaf': 50, \n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# validation\n",
    "y_val_pred_log = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "y_val_pred = np.expm1(y_val_pred_log) \n",
    "\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "rmsle = np.sqrt(mean_squared_log_error(train.loc[train['date'] >= '2017-03-01','sales'], np.maximum(y_val_pred,0)))\n",
    "\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "print(f\"Validation RMSLE: {rmsle:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c350ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T02:21:54.151500Z",
     "iopub.status.busy": "2025-08-29T02:21:54.151016Z",
     "iopub.status.idle": "2025-08-29T02:21:55.178693Z",
     "shell.execute_reply": "2025-08-29T02:21:55.177869Z"
    },
    "papermill": {
     "duration": 1.033326,
     "end_time": "2025-08-29T02:21:55.180371",
     "exception": false,
     "start_time": "2025-08-29T02:21:54.147045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "X_test = test.drop(columns=['date','id'])\n",
    "y_test_pred_log = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "\n",
    "# generate submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'sales': np.maximum(y_test_pred, 0) \n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 211.842655,
   "end_time": "2025-08-29T02:21:56.104065",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-29T02:18:24.261410",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
